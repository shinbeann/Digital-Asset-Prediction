{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys \n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aishwaryaiyer'"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the project directory \n",
    "current_dir = os.path.abspath('') # Current '\\notebooks' directory\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, '..')) # Move up one level to project root directory\n",
    "\n",
    "# Add the project directory to sys.path\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "# Move up to project directory\n",
    "os.chdir(project_dir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import *\n",
    "from src.dataset import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "\n",
    "df = pd.read_csv('/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction/data/processed/combined_dataset_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADA/USDT' 'ALGO/USDT' 'ANKR/USDT' 'ARDR/USDT' 'ARPA/USDT' 'ATOM/USDT'\n",
      " 'BAL/USDT' 'BAND/USDT' 'BAT/USDT' 'BCH/USDT' 'BNB/USDT' 'BNT/USDT'\n",
      " 'BTC/USDT' 'CELR/USDT' 'CHR/USDT' 'CHZ/USDT' 'COMP/USDT' 'COS/USDT'\n",
      " 'COTI/USDT' 'CRV/USDT' 'CTSI/USDT' 'CTXC/USDT' 'CVC/USDT' 'DASH/USDT'\n",
      " 'DATA/USDT' 'DCR/USDT' 'DENT/USDT' 'DGB/USDT' 'DOGE/USDT' 'DOT/USDT'\n",
      " 'DUSK/USDT' 'ENJ/USDT' 'EOS/USDT' 'ETC/USDT' 'ETH/USDT' 'EUR/USDT'\n",
      " 'FET/USDT' 'FTT/USDT' 'FUN/USDT' 'HBAR/USDT' 'HIVE/USDT' 'HOT/USDT'\n",
      " 'ICX/USDT' 'IOST/USDT' 'IOTA/USDT' 'IOTX/USDT' 'JST/USDT' 'KAVA/USDT'\n",
      " 'KMD/USDT' 'KNC/USDT' 'LINK/USDT' 'LRC/USDT' 'LSK/USDT' 'LTC/USDT'\n",
      " 'LTO/USDT' 'LUNA/USDT' 'MANA/USDT' 'MBL/USDT' 'MDT/USDT' 'MKR/USDT'\n",
      " 'MTL/USDT' 'NEO/USDT' 'NKN/USDT' 'NMR/USDT' 'NULS/USDT' 'OGN/USDT'\n",
      " 'ONE/USDT' 'ONG/USDT' 'ONT/USDT' 'PAXG/USDT' 'QTUM/USDT' 'RLC/USDT'\n",
      " 'RSR/USDT' 'RVN/USDT' 'SAND/USDT' 'SC/USDT' 'SNX/USDT' 'SOL/USDT'\n",
      " 'STORJ/USDT' 'STPT/USDT' 'STX/USDT' 'SXP/USDT' 'TFUEL/USDT' 'THETA/USDT'\n",
      " 'TROY/USDT' 'TRX/USDT' 'TUSD/USDT' 'USDC/USDT' 'VET/USDT' 'VTHO/USDT'\n",
      " 'WAN/USDT' 'WIN/USDT' 'XLM/USDT' 'XRP/USDT' 'XTZ/USDT' 'YFI/USDT'\n",
      " 'ZEC/USDT' 'ZEN/USDT' 'ZIL/USDT' 'ZRX/USDT']\n"
     ]
    }
   ],
   "source": [
    "unique_symbols = df['symbol'].unique()\n",
    "print(unique_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Days per symbol in final dataset:\n",
      "symbol\n",
      "ADA/USDT     14\n",
      "ALGO/USDT    14\n",
      "ANKR/USDT    14\n",
      "ARDR/USDT    14\n",
      "ARPA/USDT    14\n",
      "             ..\n",
      "YFI/USDT     14\n",
      "ZEC/USDT     14\n",
      "ZEN/USDT     14\n",
      "ZIL/USDT     14\n",
      "ZRX/USDT     14\n",
      "Length: 100, dtype: int64\n",
      "\n",
      "Successfully processed 100 out of 100 tickers\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your original dataframe\n",
    "df['date'] = pd.to_datetime(df['date'])  # Ensure date is datetime type\n",
    "df = df.sort_values(['symbol', 'date'])  # Sort by symbol and date\n",
    "\n",
    "# Function to get valid 14-day sequences ending on a specific date for a symbol\n",
    "def get_14day_sequence_ending_on(symbol_data, end_date):\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    start_date = end_date - pd.Timedelta(days=13)  # 14 days inclusive\n",
    "    \n",
    "    # Filter data for the date range\n",
    "    sequence = symbol_data[(symbol_data['date'] >= start_date) & \n",
    "                          (symbol_data['date'] <= end_date)]\n",
    "    \n",
    "    # Check if we have all 14 consecutive days\n",
    "    if len(sequence) == 14:\n",
    "        # Verify the dates are consecutive\n",
    "        date_diff = sequence['date'].diff().dropna()\n",
    "        if all(date_diff == pd.Timedelta(days=1)):\n",
    "            return sequence\n",
    "    return None\n",
    "\n",
    "# Manually specify your 20 tickers and prediction date\n",
    "selected_tickers = ['ADA/USDT', 'ALGO/USDT', 'ANKR/USDT', 'ARDR/USDT', 'ARPA/USDT', 'ATOM/USDT',\n",
    "    'BAL/USDT', 'BAND/USDT', 'BAT/USDT', 'BCH/USDT', 'BNB/USDT', 'BNT/USDT',\n",
    "    'BTC/USDT', 'CELR/USDT', 'CHR/USDT', 'CHZ/USDT', 'COMP/USDT', 'COS/USDT',\n",
    "    'COTI/USDT', 'CRV/USDT', 'CTSI/USDT', 'CTXC/USDT', 'CVC/USDT', 'DASH/USDT',\n",
    "    'DATA/USDT', 'DCR/USDT', 'DENT/USDT', 'DGB/USDT', 'DOGE/USDT', 'DOT/USDT',\n",
    "    'DUSK/USDT', 'ENJ/USDT', 'EOS/USDT', 'ETC/USDT', 'ETH/USDT', 'EUR/USDT',\n",
    "    'FET/USDT', 'FTT/USDT', 'FUN/USDT', 'HBAR/USDT', 'HIVE/USDT', 'HOT/USDT',\n",
    "    'ICX/USDT', 'IOST/USDT', 'IOTA/USDT', 'IOTX/USDT', 'JST/USDT', 'KAVA/USDT',\n",
    "    'KMD/USDT', 'KNC/USDT', 'LINK/USDT', 'LRC/USDT', 'LSK/USDT', 'LTC/USDT',\n",
    "    'LTO/USDT', 'LUNA/USDT', 'MANA/USDT', 'MBL/USDT', 'MDT/USDT', 'MKR/USDT',\n",
    "    'MTL/USDT', 'NEO/USDT', 'NKN/USDT', 'NMR/USDT', 'NULS/USDT', 'OGN/USDT',\n",
    "    'ONE/USDT', 'ONG/USDT', 'ONT/USDT', 'PAXG/USDT', 'QTUM/USDT', 'RLC/USDT',\n",
    "    'RSR/USDT', 'RVN/USDT', 'SAND/USDT', 'SC/USDT', 'SNX/USDT', 'SOL/USDT',\n",
    "    'STORJ/USDT', 'STPT/USDT', 'STX/USDT', 'SXP/USDT', 'TFUEL/USDT', 'THETA/USDT',\n",
    "    'TROY/USDT', 'TRX/USDT', 'TUSD/USDT', 'USDC/USDT', 'VET/USDT', 'VTHO/USDT',\n",
    "    'WAN/USDT', 'WIN/USDT', 'XLM/USDT', 'XRP/USDT', 'XTZ/USDT', 'YFI/USDT',\n",
    "    'ZEC/USDT', 'ZEN/USDT', 'ZIL/USDT', 'ZRX/USDT']\n",
    "\n",
    "prediction_date = '2025-03-24'  # I hard-coded this for now bc all of the values are valid for 14-days\n",
    "selected_sequences = []\n",
    "\n",
    "for ticker in selected_tickers:\n",
    "    # Get all data for this symbol\n",
    "    symbol_data = df[df['symbol'] == ticker].sort_values('date')\n",
    "    \n",
    "    # Get the 14-day sequence ending on prediction_date\n",
    "    sequence = get_14day_sequence_ending_on(symbol_data, prediction_date)\n",
    "    \n",
    "    if sequence is not None:\n",
    "        selected_sequences.append(sequence)\n",
    "    else:\n",
    "        print(f\"Warning: No valid 14-day sequence ending on {prediction_date} for {ticker}\")\n",
    "\n",
    "# Combine all valid sequences into one dataframe\n",
    "if selected_sequences:\n",
    "    final_df = pd.concat(selected_sequences)\n",
    "    \n",
    "    # Verify we have exactly 14 days per symbol\n",
    "    print(\"\\nDays per symbol in final dataset:\")\n",
    "    print(final_df.groupby('symbol').size())\n",
    "    \n",
    "    # Save to CSV if needed\n",
    "    final_df.to_csv('14day_new_crypto_sequences_custom.csv', index=False)\n",
    "    print(f\"\\nSuccessfully processed {len(selected_sequences)} out of {len(selected_tickers)} tickers\")\n",
    "else:\n",
    "    print(\"No valid sequences found for the selected tickers and date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionDataset(Dataset):\n",
    "    \"\"\"Dataset for making predictions on pre-processed 14-day windows\"\"\"\n",
    "    def __init__(self, df, feature_cols, target_col='close'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame containing the 14-day sequences\n",
    "            feature_cols: List of feature column names to use\n",
    "            target_col: Name of target column (default 'close')\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df) // 14  # Each sequence is 14 days\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * 14\n",
    "        end_idx = start_idx + 14\n",
    "        \n",
    "        # Get input sequence features\n",
    "        sequence = self.df.iloc[start_idx:end_idx][self.feature_cols].values\n",
    "        # Get target (next day's close price)\n",
    "        target = self.df.iloc[end_idx][self.target_col] if end_idx < len(self.df) else 0\n",
    "        \n",
    "        X = torch.tensor(sequence, dtype=torch.float32)\n",
    "        y = torch.tensor(target, dtype=torch.float32)\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_crypto_prices(df, transformer_model, informer_model, normalizer, batch_size=32):\n",
    "    \"\"\"\n",
    "    Make predictions using both Transformer and Informer models on 14-day crypto sequences.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing 14-day sequences for multiple cryptocurrencies\n",
    "        transformer_model: Loaded CryptoTransformer model\n",
    "        informer_model: Loaded CryptoInformer model\n",
    "        normalizer: Normalizer object fitted to training data\n",
    "        batch_size: Batch size for prediction\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predictions from both models\n",
    "    \"\"\"\n",
    "    # Ensure models are in eval mode\n",
    "    transformer_model.eval()\n",
    "    informer_model.eval()\n",
    "    \n",
    "    # Get feature columns (exclude date and symbol)\n",
    "    feature_cols = [col for col in df.columns if col not in ['date', 'symbol']]\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = PredictionDataset(df, feature_cols)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Store predictions\n",
    "    transformer_preds = []\n",
    "    informer_preds = []\n",
    "    dates = []\n",
    "    symbols = []\n",
    "    actual_closes = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            seq_batch, target_batch = batch\n",
    "            \n",
    "            # Normalize inputs\n",
    "            seq_batch = normalizer(seq_batch)\n",
    "            \n",
    "            # Get predictions from both models\n",
    "            transformer_output = transformer_model(seq_batch)\n",
    "            informer_output = informer_model(seq_batch)\n",
    "            \n",
    "            transformer_preds.extend(transformer_output.numpy())\n",
    "            informer_preds.extend(informer_output.numpy())\n",
    "    \n",
    "    # Create prediction DataFrame\n",
    "    # We'll align predictions with the last day of each 14-day window\n",
    "    prediction_points = []\n",
    "    for i in range(len(transformer_preds)):\n",
    "        idx = (i + 1) * 14 - 1  # Last index of each window\n",
    "        if idx < len(df):\n",
    "            prediction_points.append(idx)\n",
    "    \n",
    "    result_df = df.iloc[prediction_points].copy()\n",
    "    result_df['transformer_pred'] = transformer_preds[:len(prediction_points)]\n",
    "    result_df['informer_pred'] = informer_preds[:len(prediction_points)]\n",
    "    \n",
    "    # Calculate next day's actual close if available\n",
    "    result_df['next_close'] = result_df['close'].shift(-1)\n",
    "    \n",
    "    return result_df[['date', 'symbol', 'close', 'next_close', 'transformer_pred', 'informer_pred']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/qwxtvrr17fn2qzlm0z4rq4br0000gn/T/ipykernel_44580/3125865980.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  transformer_model.load_state_dict(torch.load(transformer_model_path, map_location=torch.device('cpu')))\n",
      "/var/folders/vr/qwxtvrr17fn2qzlm0z4rq4br0000gn/T/ipykernel_44580/3125865980.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  informer_model.load_state_dict(torch.load(informer_model_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to crypto_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load your models (as you've shown)\n",
    "    transformer_model_path = \"/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction/saved_models/CryptoTransformer/Best_R2.pth\"\n",
    "    informer_model_path = \"/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction/saved_models/CryptoInformer/Best_R2.pth\"\n",
    "    \n",
    "    transformer_model = CryptoTransformer()  \n",
    "    informer_model = CryptoInformer()\n",
    "    \n",
    "    transformer_model.load_state_dict(torch.load(transformer_model_path, map_location=torch.device('cpu')))\n",
    "    informer_model.load_state_dict(torch.load(informer_model_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    train_data_path = \"/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction/data/processed/train_set.csv\"\n",
    "    train_dataset = CryptoDataset(train_data_path)\n",
    "    normalizer = Normalizer()\n",
    "    normalizer.fit(train_dataset)\n",
    "\n",
    "    \n",
    "\n",
    "    # Load your input DataFrame (14-day windows for top 20 cryptos)\n",
    "    df = pd.read_csv('14day_new_crypto_sequences_custom.csv')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(['symbol', 'date'])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_df = predict_crypto_prices(df, transformer_model, informer_model, normalizer)\n",
    "    \n",
    "    # Save results\n",
    "    predictions_df.to_csv(\"crypto_predictions.csv\", index=False)\n",
    "    print(\"Predictions saved to crypto_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date     symbol       close  next_close  transformer_pred  \\\n",
      "13   2025-03-24   ADA/USDT     0.73200     0.20360         32.198696   \n",
      "27   2025-03-24  ALGO/USDT     0.20360     0.02058         33.195305   \n",
      "41   2025-03-24  ANKR/USDT     0.02058     0.06380         33.230850   \n",
      "55   2025-03-24  ARDR/USDT     0.06380     0.02994         33.293629   \n",
      "69   2025-03-24  ARPA/USDT     0.02994     4.95600         33.245804   \n",
      "...         ...        ...         ...         ...               ...   \n",
      "1343 2025-03-24   YFI/USDT  5416.00000    32.44000       3050.841064   \n",
      "1357 2025-03-24   ZEC/USDT    32.44000     9.67000         33.759171   \n",
      "1371 2025-03-24   ZEN/USDT     9.67000     0.01296         33.395817   \n",
      "1385 2025-03-24   ZIL/USDT     0.01296     0.29140         33.187714   \n",
      "1399 2025-03-24   ZRX/USDT     0.29140         NaN         33.283978   \n",
      "\n",
      "      informer_pred  \n",
      "13        10.673519  \n",
      "27         9.216707  \n",
      "41         9.129238  \n",
      "55         9.180437  \n",
      "69         9.134610  \n",
      "...             ...  \n",
      "1343    3314.342529  \n",
      "1357       9.699081  \n",
      "1371       9.363642  \n",
      "1385       9.083715  \n",
      "1399       9.188145  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['percentage_increase'] = (predictions_df['informer_pred'] - predictions_df['close']) / predictions_df['close'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R² score between next_close and transformer_pred: -0.5724\n",
      "R² score between next_close and informer_pred: -0.5957\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import r2_score\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming your DataFrame is named df\n",
    "# First, drop rows where next_close or transformer_pred is NaN\n",
    "predictions_df_clean = predictions_df.dropna(subset=['next_close', 'transformer_pred'])\n",
    "\n",
    "# Calculate R² score\n",
    "r2t = r2_score(predictions_df_clean['next_close'], predictions_df_clean['transformer_pred'])\n",
    "r2i = r2_score(predictions_df_clean['next_close'], predictions_df_clean['informer_pred'])\n",
    "\n",
    "print(f\"R² score between next_close and transformer_pred: {r2t:.4f}\")\n",
    "print(f\"R² score between next_close and informer_pred: {r2i:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date     symbol       close  next_close  transformer_pred  \\\n",
      "13   2025-03-24   ADA/USDT     0.73200     0.20360         32.198696   \n",
      "27   2025-03-24  ALGO/USDT     0.20360     0.02058         33.195305   \n",
      "41   2025-03-24  ANKR/USDT     0.02058     0.06380         33.230850   \n",
      "55   2025-03-24  ARDR/USDT     0.06380     0.02994         33.293629   \n",
      "69   2025-03-24  ARPA/USDT     0.02994     4.95600         33.245804   \n",
      "...         ...        ...         ...         ...               ...   \n",
      "1343 2025-03-24   YFI/USDT  5416.00000    32.44000       3050.841064   \n",
      "1357 2025-03-24   ZEC/USDT    32.44000     9.67000         33.759171   \n",
      "1371 2025-03-24   ZEN/USDT     9.67000     0.01296         33.395817   \n",
      "1385 2025-03-24   ZIL/USDT     0.01296     0.29140         33.187714   \n",
      "1399 2025-03-24   ZRX/USDT     0.29140         NaN         33.283978   \n",
      "\n",
      "      informer_pred  percentage_increase    proportion  percent_of_coins  \\\n",
      "13        10.673519          1358.131029  4.966509e-05               0.0   \n",
      "27         9.216707          4426.869956  1.618849e-04               0.0   \n",
      "41         9.129238         44259.757671  1.618522e-03               0.0   \n",
      "55         9.180437         14289.399824  5.225448e-04               0.0   \n",
      "69         9.134610         30409.720027  1.112044e-03               0.0   \n",
      "...             ...                  ...           ...               ...   \n",
      "1343    3314.342529           -38.804606 -1.419034e-06              -0.0   \n",
      "1357       9.699081           -70.101475 -2.563520e-06              -0.0   \n",
      "1371       9.363642            -3.168131 -1.158544e-07              -0.0   \n",
      "1385       9.083715         69990.396905  2.559458e-03               0.0   \n",
      "1399       9.188145          3053.103872  1.116480e-04               0.0   \n",
      "\n",
      "      actual_percentage_increase  actual_proportion  percent_of_coins_actual  \n",
      "13                    -72.185792          -0.000003                     -0.0  \n",
      "27                    -89.891945          -0.000003                     -0.0  \n",
      "41                    210.009718           0.000007                      0.0  \n",
      "55                    -53.072100          -0.000002                     -0.0  \n",
      "69                  16453.106212           0.000578                      0.0  \n",
      "...                          ...                ...                      ...  \n",
      "1343                  -99.401034          -0.000003                     -0.0  \n",
      "1357                  -70.191122          -0.000002                     -0.0  \n",
      "1371                  -99.865977          -0.000004                     -0.0  \n",
      "1385                 2148.456790           0.000075                      0.0  \n",
      "1399                         NaN                NaN                      NaN  \n",
      "\n",
      "[100 rows x 12 columns]\n",
      "Final predictions saved to 'final_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "# Calculate the total percentage increase (predicted)\n",
    "total_percentage_increase = predictions_df['percentage_increase'].sum()\n",
    "\n",
    "# Avoid division by zero\n",
    "if total_percentage_increase == 0:\n",
    "    raise ValueError(\"Total predicted percentage increase is zero, cannot calculate proportion.\")\n",
    "\n",
    "# Proportion and coin allocation based on predicted increase\n",
    "predictions_df['proportion'] = predictions_df['percentage_increase'] / total_percentage_increase\n",
    "predictions_df['percent_of_coins'] = (predictions_df['proportion'] * 100).round()\n",
    "\n",
    "# Adjust to ensure total = 100\n",
    "total_coins = predictions_df['percent_of_coins'].sum()\n",
    "if total_coins != 100:\n",
    "    max_error_symbol = predictions_df.loc[predictions_df['percent_of_coins'].idxmax(), 'symbol']\n",
    "    predictions_df.loc[predictions_df['symbol'] == max_error_symbol, 'percent_of_coins'] -= total_coins - 100\n",
    "\n",
    "# ---------------------------------------\n",
    "# Now calculate actual allocation\n",
    "# ---------------------------------------\n",
    "\n",
    "# Calculate actual percentage increase\n",
    "predictions_df['actual_percentage_increase'] = (\n",
    "    (predictions_df['next_close'] - predictions_df['close']) / predictions_df['close']\n",
    ") * 100\n",
    "\n",
    "# Sum of actual percentage increases\n",
    "total_actual_increase = predictions_df['actual_percentage_increase'].sum()\n",
    "\n",
    "if total_actual_increase == 0:\n",
    "    raise ValueError(\"Total actual percentage increase is zero, cannot calculate proportion.\")\n",
    "\n",
    "# Proportion and coin allocation based on actual increase\n",
    "predictions_df['actual_proportion'] = predictions_df['actual_percentage_increase'] / total_actual_increase\n",
    "predictions_df['percent_of_coins_actual'] = (predictions_df['actual_proportion'] * 100).round()\n",
    "\n",
    "# Adjust to ensure total = 100\n",
    "total_actual_coins = predictions_df['percent_of_coins_actual'].sum()\n",
    "if total_actual_coins != 100:\n",
    "    max_error_symbol_actual = predictions_df.loc[predictions_df['percent_of_coins_actual'].idxmax(), 'symbol']\n",
    "    predictions_df.loc[\n",
    "        predictions_df['symbol'] == max_error_symbol_actual, 'percent_of_coins_actual'\n",
    "    ] -= total_actual_coins - 100\n",
    "\n",
    "# Final check\n",
    "assert predictions_df['percent_of_coins'].sum() == 100, \"Predicted total is not 100\"\n",
    "assert predictions_df['percent_of_coins_actual'].sum() == 100, \"Actual total is not 100\"\n",
    "\n",
    "# Save results\n",
    "predictions_df.to_csv('final_predictions_latest.csv', index=False)\n",
    "print(predictions_df)\n",
    "\n",
    "print(\"Final predictions saved to 'final_predictions.csv'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average price: 968.32\n"
     ]
    }
   ],
   "source": [
    "\n",
    "average_price = df['close'].mean()\n",
    "\n",
    "print(f\"Average price: {average_price:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
