{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 737, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3046, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3101, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3488, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/vr/qwxtvrr17fn2qzlm0z4rq4br0000gn/T/ipykernel_74344/1464153023.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arrays/arrow/array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/ops/__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/ops/array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/computation/expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/computation/check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numexpr/__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "import random\n",
    "import sys \n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the project directory \n",
    "current_dir = os.path.abspath('') # Current '\\notebooks' directory\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, '..')) # Move up one level to project root directory\n",
    "\n",
    "# Add the project directory to sys.path\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "# Move up to project directory\n",
    "os.chdir(project_dir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import *\n",
    "from src.dataset import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Days per symbol in final dataset:\n",
      "symbol\n",
      "ALGO/USDT    14\n",
      "ANKR/USDT    14\n",
      "ARDR/USDT    14\n",
      "CHR/USDT     14\n",
      "CTXC/USDT    14\n",
      "DATA/USDT    14\n",
      "DCR/USDT     14\n",
      "EUR/USDT     14\n",
      "IOTA/USDT    14\n",
      "LRC/USDT     14\n",
      "MDT/USDT     14\n",
      "NKN/USDT     14\n",
      "RVN/USDT     14\n",
      "SNX/USDT     14\n",
      "STPT/USDT    14\n",
      "STX/USDT     14\n",
      "USDC/USDT    14\n",
      "XLM/USDT     14\n",
      "XRP/USDT     14\n",
      "ZEN/USDT     14\n",
      "dtype: int64\n",
      "Selected 20 symbols with valid 14-day sequences\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load your dataset\n",
    "df = pd.read_csv('/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction/data/processed/combined_dataset_v1.csv')\n",
    "df['date'] = pd.to_datetime(df['date'])  # Ensure date is datetime type\n",
    "df = df.sort_values(['symbol', 'date'])  # Sort by symbol and date\n",
    "\n",
    "# Function to get valid 14-day sequences for a symbol\n",
    "def get_14day_sequences(symbol_data, min_sequence_length=14):\n",
    "    sequences = []\n",
    "    current_sequence = []\n",
    "    \n",
    "    for _, row in symbol_data.iterrows():\n",
    "        if not current_sequence:\n",
    "            current_sequence.append(row)\n",
    "        else:\n",
    "            # Check if this row is consecutive to the last one in current sequence\n",
    "            last_date = current_sequence[-1]['date']\n",
    "            if row['date'] == last_date + pd.Timedelta(days=1):\n",
    "                current_sequence.append(row)\n",
    "            else:\n",
    "                # Reset if not consecutive\n",
    "                current_sequence = [row]\n",
    "        \n",
    "        # If we have enough consecutive days, save it and move window by 1 day\n",
    "        if len(current_sequence) >= min_sequence_length:\n",
    "            sequences.append(current_sequence[:min_sequence_length])\n",
    "            current_sequence = current_sequence[1:]  # Slide window by 1 day\n",
    "    \n",
    "    return sequences\n",
    "\n",
    "# Get 20 random tickers with enough data\n",
    "all_symbols = df['symbol'].unique()\n",
    "np.random.seed(42)  # For reproducibility\n",
    "\n",
    "selected_symbols = []\n",
    "selected_sequences = []\n",
    "\n",
    "# Keep trying until we get 20 symbols with valid sequences\n",
    "while len(selected_symbols) < 20 and len(all_symbols) > 0:\n",
    "    # Randomly select a symbol\n",
    "    symbol = np.random.choice(all_symbols)\n",
    "    all_symbols = all_symbols[all_symbols != symbol]  # Remove from pool\n",
    "    \n",
    "    # Get all data for this symbol\n",
    "    symbol_data = df[df['symbol'] == symbol].sort_values('date')\n",
    "    \n",
    "    # Get all valid 14-day sequences\n",
    "    sequences = get_14day_sequences(symbol_data)\n",
    "    \n",
    "    # If we found at least one valid sequence, add to our selection\n",
    "    if sequences:\n",
    "        selected_symbols.append(symbol)\n",
    "        # Take the first sequence for this symbol (you could randomize this too)\n",
    "        selected_sequences.extend(sequences[0])\n",
    "\n",
    "# Create the final DataFrame with 14-day sequences for 20 random cryptos\n",
    "final_df = pd.DataFrame(selected_sequences)\n",
    "\n",
    "# Verify we have exactly 14 days per symbol\n",
    "print(\"Days per symbol in final dataset:\")\n",
    "print(final_df.groupby('symbol').size())\n",
    "\n",
    "# Save to CSV if needed\n",
    "final_df.to_csv('14day_crypto_sequences.csv', index=False)\n",
    "print(f\"Selected {len(selected_symbols)} symbols with valid 14-day sequences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionDataset(Dataset):\n",
    "    \"\"\"Dataset for making predictions on pre-processed 14-day windows\"\"\"\n",
    "    def __init__(self, df, feature_cols, target_col='close'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame containing the 14-day sequences\n",
    "            feature_cols: List of feature column names to use\n",
    "            target_col: Name of target column (default 'close')\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df) // 14  # Each sequence is 14 days\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * 14\n",
    "        end_idx = start_idx + 14\n",
    "        \n",
    "        # Get input sequence features\n",
    "        sequence = self.df.iloc[start_idx:end_idx][self.feature_cols].values\n",
    "        # Get target (next day's close price)\n",
    "        target = self.df.iloc[end_idx][self.target_col] if end_idx < len(self.df) else 0\n",
    "        \n",
    "        X = torch.tensor(sequence, dtype=torch.float32)\n",
    "        y = torch.tensor(target, dtype=torch.float32)\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_crypto_prices(df, transformer_model, informer_model, normalizer, batch_size=32):\n",
    "    \"\"\"\n",
    "    Make predictions using both Transformer and Informer models on 14-day crypto sequences.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing 14-day sequences for multiple cryptocurrencies\n",
    "        transformer_model: Loaded CryptoTransformer model\n",
    "        informer_model: Loaded CryptoInformer model\n",
    "        normalizer: Normalizer object fitted to training data\n",
    "        batch_size: Batch size for prediction\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predictions from both models\n",
    "    \"\"\"\n",
    "    # Ensure models are in eval mode\n",
    "    transformer_model.eval()\n",
    "    informer_model.eval()\n",
    "    \n",
    "    # Get feature columns (exclude date and symbol)\n",
    "    feature_cols = [col for col in df.columns if col not in ['date', 'symbol']]\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = PredictionDataset(df, feature_cols)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Store predictions\n",
    "    transformer_preds = []\n",
    "    informer_preds = []\n",
    "    dates = []\n",
    "    symbols = []\n",
    "    actual_closes = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            seq_batch, target_batch = batch\n",
    "            \n",
    "            # Normalize inputs\n",
    "            seq_batch = normalizer(seq_batch)\n",
    "            \n",
    "            # Get predictions from both models\n",
    "            transformer_output = transformer_model(seq_batch)\n",
    "            informer_output = informer_model(seq_batch)\n",
    "            \n",
    "            transformer_preds.extend(transformer_output.numpy())\n",
    "            informer_preds.extend(informer_output.numpy())\n",
    "    \n",
    "    # Create prediction DataFrame\n",
    "    # We'll align predictions with the last day of each 14-day window\n",
    "    prediction_points = []\n",
    "    for i in range(len(transformer_preds)):\n",
    "        idx = (i + 1) * 14 - 1  # Last index of each window\n",
    "        if idx < len(df):\n",
    "            prediction_points.append(idx)\n",
    "    \n",
    "    result_df = df.iloc[prediction_points].copy()\n",
    "    result_df['transformer_pred'] = transformer_preds[:len(prediction_points)]\n",
    "    result_df['informer_pred'] = informer_preds[:len(prediction_points)]\n",
    "    \n",
    "    # Calculate next day's actual close if available\n",
    "    result_df['next_close'] = result_df['close'].shift(-1)\n",
    "    \n",
    "    return result_df[['date', 'symbol', 'close', 'next_close', 'transformer_pred', 'informer_pred']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/qwxtvrr17fn2qzlm0z4rq4br0000gn/T/ipykernel_74344/2913173267.py:10: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  transformer_model.load_state_dict(torch.load(transformer_model_path, map_location=torch.device('cpu')))\n",
      "/var/folders/vr/qwxtvrr17fn2qzlm0z4rq4br0000gn/T/ipykernel_74344/2913173267.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  informer_model.load_state_dict(torch.load(informer_model_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to crypto_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "    # Load your models (as you've shown)\n",
    "    transformer_model_path = \"/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction/saved_models/CryptoTransformer_2025-04-09_21-31-23/CryptoTransformer_BEST_R2.pth\"\n",
    "    informer_model_path = \"/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction/saved_models/CryptoInformer_2025-04-10_15-11-19/CryptoInformer_BEST_R2.pth\"\n",
    "    \n",
    "    transformer_model = CryptoTransformer()  \n",
    "    informer_model = CryptoInformer()\n",
    "    \n",
    "    transformer_model.load_state_dict(torch.load(transformer_model_path, map_location=torch.device('cpu')))\n",
    "    informer_model.load_state_dict(torch.load(informer_model_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    train_data_path = \"/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction/data/processed/combined_dataset_v1.csv\"\n",
    "    train_dataset = CryptoDataset(train_data_path)\n",
    "    normalizer = Normalizer()\n",
    "    normalizer.fit(train_dataset)\n",
    "\n",
    "    # Load your input DataFrame (14-day windows for top 20 cryptos)\n",
    "    df = pd.read_csv('14day_crypto_sequences.csv')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(['symbol', 'date'])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_df = predict_crypto_prices(df, transformer_model, informer_model, normalizer)\n",
    "    \n",
    "    # Save results\n",
    "    predictions_df.to_csv(\"crypto_predictions.csv\", index=False)\n",
    "    print(\"Predictions saved to crypto_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date     symbol     close  next_close  transformer_pred  \\\n",
      "223 2022-04-06  ALGO/USDT   0.76610     0.07738         -3.402395   \n",
      "181 2022-04-06  ANKR/USDT   0.07738     0.21540         -3.641128   \n",
      "265 2022-04-06  ARDR/USDT   0.21540     0.49450         -3.384028   \n",
      "41  2022-04-06   CHR/USDT   0.49450     0.27320         -3.575320   \n",
      "83  2022-04-06  CTXC/USDT   0.27320     0.07859         -3.526515   \n",
      "195 2022-04-06  DATA/USDT   0.07859    58.70000         -3.677765   \n",
      "167 2022-04-06   DCR/USDT  58.70000     1.08800         -2.926293   \n",
      "237 2022-04-06   EUR/USDT   1.08800     0.73600         -3.337085   \n",
      "251 2022-04-06  IOTA/USDT   0.73600     0.96680         -3.359430   \n",
      "13  2022-04-06   LRC/USDT   0.96680     0.05448         -3.539128   \n",
      "209 2022-04-06   MDT/USDT   0.05448     0.23410         -3.630266   \n",
      "69  2022-04-06   NKN/USDT   0.23410     0.05914         -3.453286   \n",
      "55  2022-04-06   RVN/USDT   0.05914     5.38400         -3.891540   \n",
      "279 2022-04-06   SNX/USDT   5.38400     0.09623         -3.064462   \n",
      "125 2022-04-06  STPT/USDT   0.09623     1.25800         -3.393282   \n",
      "139 2022-04-06   STX/USDT   1.25800     0.99990         -3.372587   \n",
      "97  2022-04-06  USDC/USDT   0.99990     0.20310         -3.451695   \n",
      "111 2022-04-06   XLM/USDT   0.20310     0.75950         -3.525762   \n",
      "27  2022-04-06   XRP/USDT   0.75950    41.30000         -3.483998   \n",
      "153 2022-04-06   ZEN/USDT  41.30000         NaN         -3.024539   \n",
      "\n",
      "     informer_pred  \n",
      "223     106.787476  \n",
      "181      96.706558  \n",
      "265     109.913956  \n",
      "41      105.676437  \n",
      "83      107.894913  \n",
      "195     106.390190  \n",
      "167     117.530830  \n",
      "237     106.707726  \n",
      "251     108.668571  \n",
      "13      101.761246  \n",
      "209     100.901451  \n",
      "69      108.542183  \n",
      "55       95.591934  \n",
      "279     109.453651  \n",
      "125     108.687477  \n",
      "139     109.984787  \n",
      "97       99.407585  \n",
      "111     102.389412  \n",
      "27       96.446640  \n",
      "153     115.728889  \n"
     ]
    }
   ],
   "source": [
    "print(predictions_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
