{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "A module that was compiled using NumPy 1.x cannot be run in\n",
      "NumPy 2.0.1 as it may crash. To support both 1.x and 2.x\n",
      "versions of NumPy, modules must be compiled with NumPy 2.0.\n",
      "Some module may need to rebuild instead e.g. with 'pybind11>=2.12'.\n",
      "\n",
      "If you are a user of the module, the easiest solution will be to\n",
      "downgrade to 'numpy<2' or try to upgrade the affected module.\n",
      "We expect that some modules will need time to support NumPy 2.\n",
      "\n",
      "Traceback (most recent call last):  File \"<frozen runpy>\", line 198, in _run_module_as_main\n",
      "  File \"<frozen runpy>\", line 88, in _run_code\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel_launcher.py\", line 17, in <module>\n",
      "    app.launch_new_instance()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/traitlets/config/application.py\", line 1053, in launch_instance\n",
      "    app.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/kernelapp.py\", line 737, in start\n",
      "    self.io_loop.start()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/tornado/platform/asyncio.py\", line 195, in start\n",
      "    self.asyncio_loop.run_forever()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 607, in run_forever\n",
      "    self._run_once()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/base_events.py\", line 1922, in _run_once\n",
      "    handle._run()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/asyncio/events.py\", line 80, in _run\n",
      "    self._context.run(self._callback, *self._args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 524, in dispatch_queue\n",
      "    await self.process_one()\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 513, in process_one\n",
      "    await dispatch(*args)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 418, in dispatch_shell\n",
      "    await result\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/kernelbase.py\", line 758, in execute_request\n",
      "    reply_content = await reply_content\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 426, in do_execute\n",
      "    res = shell.run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n",
      "    return super().run_cell(*args, **kwargs)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3046, in run_cell\n",
      "    result = self._run_cell(\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3101, in _run_cell\n",
      "    result = runner(coro)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n",
      "    coro.send(None)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3306, in run_cell_async\n",
      "    has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3488, in run_ast_nodes\n",
      "    if await self.run_code(code, result, async_=asy):\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/IPython/core/interactiveshell.py\", line 3548, in run_code\n",
      "    exec(code_obj, self.user_global_ns, self.user_ns)\n",
      "  File \"/var/folders/vr/qwxtvrr17fn2qzlm0z4rq4br0000gn/T/ipykernel_33443/3732787456.py\", line 1, in <module>\n",
      "    import pandas as pd\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/__init__.py\", line 49, in <module>\n",
      "    from pandas.core.api import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/api.py\", line 28, in <module>\n",
      "    from pandas.core.arrays import Categorical\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arrays/__init__.py\", line 1, in <module>\n",
      "    from pandas.core.arrays.arrow import ArrowExtensionArray\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arrays/arrow/__init__.py\", line 5, in <module>\n",
      "    from pandas.core.arrays.arrow.array import ArrowExtensionArray\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/arrays/arrow/array.py\", line 50, in <module>\n",
      "    from pandas.core import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/ops/__init__.py\", line 8, in <module>\n",
      "    from pandas.core.ops.array_ops import (\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/ops/array_ops.py\", line 56, in <module>\n",
      "    from pandas.core.computation import expressions\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/computation/expressions.py\", line 21, in <module>\n",
      "    from pandas.core.computation.check import NUMEXPR_INSTALLED\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/core/computation/check.py\", line 5, in <module>\n",
      "    ne = import_optional_dependency(\"numexpr\", errors=\"warn\")\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/pandas/compat/_optional.py\", line 135, in import_optional_dependency\n",
      "    module = importlib.import_module(name)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/numexpr/__init__.py\", line 24, in <module>\n",
      "    from numexpr.interpreter import MAX_THREADS, use_vml, __BLOCK_SIZE1__\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "_ARRAY_API not found",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: _ARRAY_API not found"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from torchsummary import summary\n",
    "from pathlib import Path\n",
    "import random\n",
    "import sys \n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/aishwaryaiyer/Documents/GitHub'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the project directory \n",
    "current_dir = os.path.abspath('') # Current '\\notebooks' directory\n",
    "project_dir = os.path.abspath(os.path.join(current_dir, '..')) # Move up one level to project root directory\n",
    "\n",
    "# Add the project directory to sys.path\n",
    "sys.path.append(project_dir)\n",
    "\n",
    "# Move up to project directory\n",
    "os.chdir(project_dir)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.models import *\n",
    "from src.dataset import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load your dataset\n",
    "\n",
    "df = pd.read_csv('/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction/data/processed/combined_dataset_v1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ADA/USDT' 'ALGO/USDT' 'ANKR/USDT' 'ARDR/USDT' 'ARPA/USDT' 'ATOM/USDT'\n",
      " 'BAL/USDT' 'BAND/USDT' 'BAT/USDT' 'BCH/USDT' 'BNB/USDT' 'BNT/USDT'\n",
      " 'BTC/USDT' 'CELR/USDT' 'CHR/USDT' 'CHZ/USDT' 'COMP/USDT' 'COS/USDT'\n",
      " 'COTI/USDT' 'CRV/USDT' 'CTSI/USDT' 'CTXC/USDT' 'CVC/USDT' 'DASH/USDT'\n",
      " 'DATA/USDT' 'DCR/USDT' 'DENT/USDT' 'DGB/USDT' 'DOGE/USDT' 'DOT/USDT'\n",
      " 'DUSK/USDT' 'ENJ/USDT' 'EOS/USDT' 'ETC/USDT' 'ETH/USDT' 'EUR/USDT'\n",
      " 'FET/USDT' 'FTT/USDT' 'FUN/USDT' 'HBAR/USDT' 'HIVE/USDT' 'HOT/USDT'\n",
      " 'ICX/USDT' 'IOST/USDT' 'IOTA/USDT' 'IOTX/USDT' 'JST/USDT' 'KAVA/USDT'\n",
      " 'KMD/USDT' 'KNC/USDT' 'LINK/USDT' 'LRC/USDT' 'LSK/USDT' 'LTC/USDT'\n",
      " 'LTO/USDT' 'LUNA/USDT' 'MANA/USDT' 'MBL/USDT' 'MDT/USDT' 'MKR/USDT'\n",
      " 'MTL/USDT' 'NEO/USDT' 'NKN/USDT' 'NMR/USDT' 'NULS/USDT' 'OGN/USDT'\n",
      " 'ONE/USDT' 'ONG/USDT' 'ONT/USDT' 'PAXG/USDT' 'QTUM/USDT' 'RLC/USDT'\n",
      " 'RSR/USDT' 'RVN/USDT' 'SAND/USDT' 'SC/USDT' 'SNX/USDT' 'SOL/USDT'\n",
      " 'STORJ/USDT' 'STPT/USDT' 'STX/USDT' 'SXP/USDT' 'TFUEL/USDT' 'THETA/USDT'\n",
      " 'TROY/USDT' 'TRX/USDT' 'TUSD/USDT' 'USDC/USDT' 'VET/USDT' 'VTHO/USDT'\n",
      " 'WAN/USDT' 'WIN/USDT' 'XLM/USDT' 'XRP/USDT' 'XTZ/USDT' 'YFI/USDT'\n",
      " 'ZEC/USDT' 'ZEN/USDT' 'ZIL/USDT' 'ZRX/USDT']\n"
     ]
    }
   ],
   "source": [
    "unique_symbols = df['symbol'].unique()\n",
    "print(unique_symbols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Days per symbol in final dataset:\n",
      "symbol\n",
      "ADA/USDT     14\n",
      "ALGO/USDT    14\n",
      "ANKR/USDT    14\n",
      "ARDR/USDT    14\n",
      "ARPA/USDT    14\n",
      "             ..\n",
      "YFI/USDT     14\n",
      "ZEC/USDT     14\n",
      "ZEN/USDT     14\n",
      "ZIL/USDT     14\n",
      "ZRX/USDT     14\n",
      "Length: 100, dtype: int64\n",
      "\n",
      "Successfully processed 100 out of 100 tickers\n"
     ]
    }
   ],
   "source": [
    "# Assuming df is your original dataframe\n",
    "df['date'] = pd.to_datetime(df['date'])  # Ensure date is datetime type\n",
    "df = df.sort_values(['symbol', 'date'])  # Sort by symbol and date\n",
    "\n",
    "# Function to get valid 14-day sequences ending on a specific date for a symbol\n",
    "def get_14day_sequence_ending_on(symbol_data, end_date):\n",
    "    end_date = pd.to_datetime(end_date)\n",
    "    start_date = end_date - pd.Timedelta(days=13)  # 14 days inclusive\n",
    "    \n",
    "    # Filter data for the date range\n",
    "    sequence = symbol_data[(symbol_data['date'] >= start_date) & \n",
    "                          (symbol_data['date'] <= end_date)]\n",
    "    \n",
    "    # Check if we have all 14 consecutive days\n",
    "    if len(sequence) == 14:\n",
    "        # Verify the dates are consecutive\n",
    "        date_diff = sequence['date'].diff().dropna()\n",
    "        if all(date_diff == pd.Timedelta(days=1)):\n",
    "            return sequence\n",
    "    return None\n",
    "\n",
    "# Manually specify your 20 tickers and prediction date\n",
    "selected_tickers = ['ADA/USDT', 'ALGO/USDT', 'ANKR/USDT', 'ARDR/USDT', 'ARPA/USDT', 'ATOM/USDT',\n",
    "    'BAL/USDT', 'BAND/USDT', 'BAT/USDT', 'BCH/USDT', 'BNB/USDT', 'BNT/USDT',\n",
    "    'BTC/USDT', 'CELR/USDT', 'CHR/USDT', 'CHZ/USDT', 'COMP/USDT', 'COS/USDT',\n",
    "    'COTI/USDT', 'CRV/USDT', 'CTSI/USDT', 'CTXC/USDT', 'CVC/USDT', 'DASH/USDT',\n",
    "    'DATA/USDT', 'DCR/USDT', 'DENT/USDT', 'DGB/USDT', 'DOGE/USDT', 'DOT/USDT',\n",
    "    'DUSK/USDT', 'ENJ/USDT', 'EOS/USDT', 'ETC/USDT', 'ETH/USDT', 'EUR/USDT',\n",
    "    'FET/USDT', 'FTT/USDT', 'FUN/USDT', 'HBAR/USDT', 'HIVE/USDT', 'HOT/USDT',\n",
    "    'ICX/USDT', 'IOST/USDT', 'IOTA/USDT', 'IOTX/USDT', 'JST/USDT', 'KAVA/USDT',\n",
    "    'KMD/USDT', 'KNC/USDT', 'LINK/USDT', 'LRC/USDT', 'LSK/USDT', 'LTC/USDT',\n",
    "    'LTO/USDT', 'LUNA/USDT', 'MANA/USDT', 'MBL/USDT', 'MDT/USDT', 'MKR/USDT',\n",
    "    'MTL/USDT', 'NEO/USDT', 'NKN/USDT', 'NMR/USDT', 'NULS/USDT', 'OGN/USDT',\n",
    "    'ONE/USDT', 'ONG/USDT', 'ONT/USDT', 'PAXG/USDT', 'QTUM/USDT', 'RLC/USDT',\n",
    "    'RSR/USDT', 'RVN/USDT', 'SAND/USDT', 'SC/USDT', 'SNX/USDT', 'SOL/USDT',\n",
    "    'STORJ/USDT', 'STPT/USDT', 'STX/USDT', 'SXP/USDT', 'TFUEL/USDT', 'THETA/USDT',\n",
    "    'TROY/USDT', 'TRX/USDT', 'TUSD/USDT', 'USDC/USDT', 'VET/USDT', 'VTHO/USDT',\n",
    "    'WAN/USDT', 'WIN/USDT', 'XLM/USDT', 'XRP/USDT', 'XTZ/USDT', 'YFI/USDT',\n",
    "    'ZEC/USDT', 'ZEN/USDT', 'ZIL/USDT', 'ZRX/USDT']\n",
    "\n",
    "prediction_date = '2025-03-24'  # I hard-coded this for now bc all of the values are valid for 14-days\n",
    "selected_sequences = []\n",
    "\n",
    "for ticker in selected_tickers:\n",
    "    # Get all data for this symbol\n",
    "    symbol_data = df[df['symbol'] == ticker].sort_values('date')\n",
    "    \n",
    "    # Get the 14-day sequence ending on prediction_date\n",
    "    sequence = get_14day_sequence_ending_on(symbol_data, prediction_date)\n",
    "    \n",
    "    if sequence is not None:\n",
    "        selected_sequences.append(sequence)\n",
    "    else:\n",
    "        print(f\"Warning: No valid 14-day sequence ending on {prediction_date} for {ticker}\")\n",
    "\n",
    "# Combine all valid sequences into one dataframe\n",
    "if selected_sequences:\n",
    "    final_df = pd.concat(selected_sequences)\n",
    "    \n",
    "    # Verify we have exactly 14 days per symbol\n",
    "    print(\"\\nDays per symbol in final dataset:\")\n",
    "    print(final_df.groupby('symbol').size())\n",
    "    \n",
    "    # Save to CSV if needed\n",
    "    final_df.to_csv('14day_new_crypto_sequences_custom.csv', index=False)\n",
    "    print(f\"\\nSuccessfully processed {len(selected_sequences)} out of {len(selected_tickers)} tickers\")\n",
    "else:\n",
    "    print(\"No valid sequences found for the selected tickers and date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PredictionDataset(Dataset):\n",
    "    \"\"\"Dataset for making predictions on pre-processed 14-day windows\"\"\"\n",
    "    def __init__(self, df, feature_cols, target_col='close'):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            df: DataFrame containing the 14-day sequences\n",
    "            feature_cols: List of feature column names to use\n",
    "            target_col: Name of target column (default 'close')\n",
    "        \"\"\"\n",
    "        self.df = df\n",
    "        self.feature_cols = feature_cols\n",
    "        self.target_col = target_col\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.df) // 14  # Each sequence is 14 days\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        start_idx = idx * 14\n",
    "        end_idx = start_idx + 14\n",
    "        \n",
    "        # Get input sequence features\n",
    "        sequence = self.df.iloc[start_idx:end_idx][self.feature_cols].values\n",
    "        # Get target (next day's close price)\n",
    "        target = self.df.iloc[end_idx][self.target_col] if end_idx < len(self.df) else 0\n",
    "        \n",
    "        X = torch.tensor(sequence, dtype=torch.float32)\n",
    "        y = torch.tensor(target, dtype=torch.float32)\n",
    "        return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def predict_crypto_prices(df, transformer_model, informer_model, normalizer, batch_size=32):\n",
    "    \"\"\"\n",
    "    Make predictions using both Transformer and Informer models on 14-day crypto sequences.\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing 14-day sequences for multiple cryptocurrencies\n",
    "        transformer_model: Loaded CryptoTransformer model\n",
    "        informer_model: Loaded CryptoInformer model\n",
    "        normalizer: Normalizer object fitted to training data\n",
    "        batch_size: Batch size for prediction\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with predictions from both models\n",
    "    \"\"\"\n",
    "    # Ensure models are in eval mode\n",
    "    transformer_model.eval()\n",
    "    informer_model.eval()\n",
    "    \n",
    "    # Get feature columns (exclude date and symbol)\n",
    "    feature_cols = [col for col in df.columns if col not in ['date', 'symbol']]\n",
    "    \n",
    "    # Create dataset and dataloader\n",
    "    dataset = PredictionDataset(df, feature_cols)\n",
    "    dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    # Store predictions\n",
    "    transformer_preds = []\n",
    "    informer_preds = []\n",
    "    dates = []\n",
    "    symbols = []\n",
    "    actual_closes = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            seq_batch, target_batch = batch\n",
    "            \n",
    "            # Normalize inputs\n",
    "            seq_batch = normalizer(seq_batch)\n",
    "            \n",
    "            # Get predictions from both models\n",
    "            transformer_output = transformer_model(seq_batch)\n",
    "            informer_output = informer_model(seq_batch)\n",
    "            \n",
    "            transformer_preds.extend(transformer_output.numpy())\n",
    "            informer_preds.extend(informer_output.numpy())\n",
    "    \n",
    "    # Create prediction DataFrame\n",
    "    # We'll align predictions with the last day of each 14-day window\n",
    "    prediction_points = []\n",
    "    for i in range(len(transformer_preds)):\n",
    "        idx = (i + 1) * 14 - 1  # Last index of each window\n",
    "        if idx < len(df):\n",
    "            prediction_points.append(idx)\n",
    "    \n",
    "    result_df = df.iloc[prediction_points].copy()\n",
    "    result_df['transformer_pred'] = transformer_preds[:len(prediction_points)]\n",
    "    result_df['informer_pred'] = informer_preds[:len(prediction_points)]\n",
    "    \n",
    "    # Calculate next day's actual close if available\n",
    "    result_df['next_close'] = result_df['close'].shift(-1)\n",
    "    \n",
    "    return result_df[['date', 'symbol', 'close', 'next_close', 'transformer_pred', 'informer_pred']]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/qwxtvrr17fn2qzlm0z4rq4br0000gn/T/ipykernel_32058/453627190.py:11: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  transformer_model.load_state_dict(torch.load(transformer_model_path, map_location=torch.device('cpu')))\n",
      "/var/folders/vr/qwxtvrr17fn2qzlm0z4rq4br0000gn/T/ipykernel_32058/453627190.py:12: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  informer_model.load_state_dict(torch.load(informer_model_path, map_location=torch.device('cpu')))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predictions saved to crypto_predictions.csv\n"
     ]
    }
   ],
   "source": [
    "# Example usage:\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # Load your models (as you've shown)\n",
    "    transformer_model_path = \"/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction/saved_models/CryptoTransformer/Best_R2.pth\"\n",
    "    informer_model_path = \"/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction/saved_models/CryptoInformer/Best_R2.pth\"\n",
    "    \n",
    "    transformer_model = CryptoTransformer()  \n",
    "    informer_model = CryptoInformer()\n",
    "    \n",
    "    transformer_model.load_state_dict(torch.load(transformer_model_path, map_location=torch.device('cpu')))\n",
    "    informer_model.load_state_dict(torch.load(informer_model_path, map_location=torch.device('cpu')))\n",
    "    \n",
    "    train_data_path = \"/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction/data/processed/train_set.csv\"\n",
    "    train_dataset = CryptoDataset(train_data_path)\n",
    "    normalizer = Normalizer()\n",
    "    normalizer.fit(train_dataset)\n",
    "\n",
    "    # Load your input DataFrame (14-day windows for top 20 cryptos)\n",
    "    df = pd.read_csv('14day_new_crypto_sequences_custom.csv')\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values(['symbol', 'date'])\n",
    "    \n",
    "    # Make predictions\n",
    "    predictions_df = predict_crypto_prices(df, transformer_model, informer_model, normalizer)\n",
    "    \n",
    "    # Save results\n",
    "    predictions_df.to_csv(\"crypto_predictions.csv\", index=False)\n",
    "    print(\"Predictions saved to crypto_predictions.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date     symbol       close  next_close  transformer_pred  \\\n",
      "13   2025-03-24   ADA/USDT     0.73200     0.20360         32.198696   \n",
      "27   2025-03-24  ALGO/USDT     0.20360     0.02058         33.195305   \n",
      "41   2025-03-24  ANKR/USDT     0.02058     0.06380         33.230850   \n",
      "55   2025-03-24  ARDR/USDT     0.06380     0.02994         33.293629   \n",
      "69   2025-03-24  ARPA/USDT     0.02994     4.95600         33.245804   \n",
      "...         ...        ...         ...         ...               ...   \n",
      "1343 2025-03-24   YFI/USDT  5416.00000    32.44000       3050.841064   \n",
      "1357 2025-03-24   ZEC/USDT    32.44000     9.67000         33.759171   \n",
      "1371 2025-03-24   ZEN/USDT     9.67000     0.01296         33.395817   \n",
      "1385 2025-03-24   ZIL/USDT     0.01296     0.29140         33.187714   \n",
      "1399 2025-03-24   ZRX/USDT     0.29140         NaN         33.283978   \n",
      "\n",
      "      informer_pred  \n",
      "13        10.673517  \n",
      "27         9.216707  \n",
      "41         9.129242  \n",
      "55         9.180437  \n",
      "69         9.134610  \n",
      "...             ...  \n",
      "1343    3314.342529  \n",
      "1357       9.699083  \n",
      "1371       9.363642  \n",
      "1385       9.083714  \n",
      "1399       9.188145  \n",
      "\n",
      "[100 rows x 6 columns]\n"
     ]
    }
   ],
   "source": [
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions_df['percentage_increase'] = (predictions_df['informer_pred'] - predictions_df['close']) / predictions_df['close'] * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date     symbol       close  next_close  transformer_pred  \\\n",
      "13   2025-03-24   ADA/USDT     0.73200     0.20360         32.198696   \n",
      "27   2025-03-24  ALGO/USDT     0.20360     0.02058         33.195305   \n",
      "41   2025-03-24  ANKR/USDT     0.02058     0.06380         33.230850   \n",
      "55   2025-03-24  ARDR/USDT     0.06380     0.02994         33.293629   \n",
      "69   2025-03-24  ARPA/USDT     0.02994     4.95600         33.245804   \n",
      "...         ...        ...         ...         ...               ...   \n",
      "1343 2025-03-24   YFI/USDT  5416.00000    32.44000       3050.841064   \n",
      "1357 2025-03-24   ZEC/USDT    32.44000     9.67000         33.759171   \n",
      "1371 2025-03-24   ZEN/USDT     9.67000     0.01296         33.395817   \n",
      "1385 2025-03-24   ZIL/USDT     0.01296     0.29140         33.187714   \n",
      "1399 2025-03-24   ZRX/USDT     0.29140         NaN         33.283978   \n",
      "\n",
      "      informer_pred  percentage_increase  \n",
      "13        10.673517          1358.130769  \n",
      "27         9.216707          4426.869956  \n",
      "41         9.129242         44259.776207  \n",
      "55         9.180437         14289.399824  \n",
      "69         9.134610         30409.720027  \n",
      "...             ...                  ...  \n",
      "1343    3314.342529           -38.804606  \n",
      "1357       9.699083           -70.101469  \n",
      "1371       9.363642            -3.168131  \n",
      "1385       9.083714         69990.389546  \n",
      "1399       9.188145          3053.103872  \n",
      "\n",
      "[100 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "print(predictions_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final predictions saved to 'final_predictions.csv'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Calculate the total percentage increase\n",
    "total_percentage_increase = predictions_df['percentage_increase'].sum()\n",
    "\n",
    "# if total_percentage_increase == 0:\n",
    "#     raise ValueError(\"Total percentage increase is zero, cannot calculate proportion.\")\n",
    "\n",
    "# Calculate the proportion for each symbol\n",
    "predictions_df['proportion'] = predictions_df['percentage_increase'] / total_percentage_increase\n",
    "\n",
    "# Calculate the number of coins for each symbol\n",
    "predictions_df['percent_of_coins'] = (predictions_df['proportion'] * 100).round()\n",
    "\n",
    "# Adjust the number of coins to ensure the total is 100\n",
    "total_coins = predictions_df['percent_of_coins'].sum()\n",
    "\n",
    "if total_coins != 100:\n",
    "    # Find the symbol with the largest rounding error\n",
    "    max_error_symbol = predictions_df.loc[predictions_df['percent_of_coins'].idxmax(), 'symbol']\n",
    "    # Adjust the number of coins for the symbol with the largest rounding error\n",
    "    predictions_df.loc[predictions_df['symbol'] == max_error_symbol, 'percent_of_coins'] -= total_coins - 100\n",
    "\n",
    "# Verify that the total number of coins is 100\n",
    "total_coins = predictions_df['percent_of_coins'].sum()\n",
    "assert total_coins == 100, \"Total number of coins is not 100\"\n",
    "\n",
    "predictions_df.to_csv(project_root/'final_predictions.csv', index=False)\n",
    "\n",
    "print(\"Final predictions saved to 'final_predictions.csv'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           date     symbol       close  next_close  transformer_pred  \\\n",
      "13   2025-03-24   ADA/USDT     0.73200     0.20360         -8.037679   \n",
      "27   2025-03-24  ALGO/USDT     0.20360     0.02058         -6.783334   \n",
      "41   2025-03-24  ANKR/USDT     0.02058     0.06380         -8.259130   \n",
      "55   2025-03-24  ARDR/USDT     0.06380     0.02994         -6.551928   \n",
      "69   2025-03-24  ARPA/USDT     0.02994     4.95600         -7.115588   \n",
      "...         ...        ...         ...         ...               ...   \n",
      "1343 2025-03-24   YFI/USDT  5416.00000    32.44000       3040.845947   \n",
      "1357 2025-03-24   ZEC/USDT    32.44000     9.67000         -5.993320   \n",
      "1371 2025-03-24   ZEN/USDT     9.67000     0.01296         -6.404458   \n",
      "1385 2025-03-24   ZIL/USDT     0.01296     0.29140         -7.215162   \n",
      "1399 2025-03-24   ZRX/USDT     0.29140         NaN         -6.636563   \n",
      "\n",
      "      informer_pred  percentage_increase    proportion  percent_of_coins  \n",
      "13         9.188528          1155.263396  2.282873e-05               0.0  \n",
      "27         6.038890          2866.056170  5.663506e-05               0.0  \n",
      "41         7.980579         38678.323126  7.643078e-04               0.0  \n",
      "55         5.727973          8878.013335  1.754351e-04               0.0  \n",
      "69         6.122960         20350.766913  4.021439e-04               0.0  \n",
      "...             ...                  ...           ...               ...  \n",
      "1343    4734.952148           -12.574739 -2.484847e-07              -0.0  \n",
      "1357       6.034493           -81.397987 -1.608475e-06              -0.0  \n",
      "1371       5.809328           -39.924223 -7.889276e-07              -0.0  \n",
      "1385       6.771086         52146.035617  1.030438e-03               0.0  \n",
      "1399       5.717358          1862.030760  3.679489e-05               0.0  \n",
      "\n",
      "[100 rows x 9 columns]\n"
     ]
    }
   ],
   "source": [
    "print(predictions_df)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
