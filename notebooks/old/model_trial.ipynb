{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_file_path = \"/Users/aishwaryaiyer/Documents/GitHub/Digital-Asset-Prediction/data/processed/combined_dataset_v1.csv\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "\n",
    "    \"\"\" \n",
    "    I am sorting it by first symbol and then date bc I think that it ensures that each asset history is maintained, that way learning is not disrupted\n",
    "    \"\"\"\n",
    "\n",
    "    df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "    df.sort_values(by=['symbol', 'date'], inplace=True)\n",
    "    return df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define dataset class\n",
    "class TimeSeriesDataset(Dataset):\n",
    "    def __init__(self, data, target_col, window_size):\n",
    "        self.data = data\n",
    "        self.target_col = target_col\n",
    "        self.window_size = window_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data) - self.window_size\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        x = self.data[index:index+self.window_size].values\n",
    "        y = self.data.iloc[index+self.window_size][self.target_col]\n",
    "        return torch.tensor(x, dtype=torch.float32), torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformer model\n",
    "class transformer(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim, num_layers=2):\n",
    "        super(transformer, self).__init__()\n",
    "        self.encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=input_dim, nhead=2), num_layers=num_layers\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=input_dim, nhead=2), num_layers=num_layers\n",
    "        )\n",
    "        self.fc = nn.Linear(input_dim, output_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        memory = self.encoder(x)\n",
    "        x = self.decoder(x, memory)\n",
    "        x = self.fc(x[:, -1, :])  # Use last time step for prediction\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, dataloader, epochs=10, lr=0.001):\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    for epoch in range(epochs):\n",
    "        for x, y in dataloader:\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y.unsqueeze(1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages/torch/nn/modules/transformer.py:307: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
      "  warnings.warn(f\"enable_nested_tensor is True, but self.use_nested_tensor is False because {why_not_sparsity_fast_path}\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Loss: 0.0194\n",
      "Epoch 2, Loss: 1.7930\n",
      "Epoch 3, Loss: 0.0134\n",
      "Epoch 4, Loss: 0.0023\n",
      "Epoch 5, Loss: 0.0041\n",
      "Epoch 6, Loss: 0.0140\n",
      "Epoch 7, Loss: 0.0093\n",
      "Epoch 8, Loss: 0.0029\n",
      "Epoch 9, Loss: 0.0068\n",
      "Epoch 10, Loss: 0.0018\n"
     ]
    }
   ],
   "source": [
    "df = load_data(data_file_path)\n",
    "df['symbol'] = df['symbol'].astype('category').cat.codesv\n",
    "\n",
    "\n",
    "# Normalize and prepare dataset\n",
    "features = ['symbol','open', 'high', 'low', 'close', 'volume', 'market_cap', 'daily_return', 'sp500', 'treasury_spread', 'fear_greed', 'gold_price_usd']\n",
    "df[features] = (df[features] - df[features].mean()) / df[features].std()\n",
    "dataset = TimeSeriesDataset(df[features], target_col='close', window_size=14)\n",
    "dataloader = DataLoader(dataset, batch_size=16, shuffle=True)\n",
    "\n",
    "# Initialize and train model\n",
    "input_dim = len(features)\n",
    "model = transfomer(input_dim, hidden_dim=64, output_dim=1)\n",
    "train_model(model, dataloader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evaluate_model(model, dataloader, plot_samples=3):\n",
    "    \"\"\"\n",
    "    Evaluate the model on test data and provide performance metrics and visualizations\n",
    "    \n",
    "    Args:\n",
    "        model: Trained PyTorch model\n",
    "        dataloader: DataLoader containing test data\n",
    "        plot_samples: Number of sample predictions to plot\n",
    "    \"\"\"\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    predictions = []\n",
    "    actuals = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for x, y in dataloader:\n",
    "            y_pred = model(x)\n",
    "            predictions.extend(y_pred.squeeze().tolist())\n",
    "            actuals.extend(y.tolist())\n",
    "    \n",
    "    # Convert to numpy arrays for easier calculations\n",
    "    predictions = np.array(predictions)\n",
    "    actuals = np.array(actuals)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    mse = mean_squared_error(actuals, predictions)\n",
    "    rmse = np.sqrt(mse)\n",
    "    mae = mean_absolute_error(actuals, predictions)\n",
    "    r2 = r2_score(actuals, predictions)\n",
    "    \n",
    "    print(f\"Evaluation Metrics:\")\n",
    "    print(f\"MSE: {mse:.4f}\")\n",
    "    print(f\"RMSE: {rmse:.4f}\")\n",
    "    print(f\"MAE: {mae:.4f}\")\n",
    "    print(f\"R² Score: {r2:.4f}\")\n",
    "    \n",
    "  \n",
    "    return {'mse': mse, 'rmse': rmse, 'mae': mae, 'r2': r2}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation Metrics:\n",
      "MSE: 0.0928\n",
      "RMSE: 0.3047\n",
      "MAE: 0.1339\n",
      "R² Score: 0.2255\n"
     ]
    }
   ],
   "source": [
    "metrics = evaluate_model(model, test_dataloader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "aishwaryaiyer-JArDMLs9",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
